{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<figure>\n",
    "<img src=\"../Imagenes/logo-final-ap.png\"  width=\"80\" height=\"80\" align=\"left\"/> \n",
    "</figure>\n",
    "\n",
    "# <span style=\"color:blue\"><left>Aprendizaje Profundo</left></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"><center>Atenci贸n y auto-atenci贸n</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<center>All you need is attention!</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Profesores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Alvaro Mauricio Montenegro D铆az, ammontenegrod@unal.edu.co\n",
    "2. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com \n",
    "3. Campo El铆as Pardo Turriago, cepardot@unal.edu.co "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Asesora Medios y Marketing digital</span>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Maria del Pilar Montenegro, pmontenegro88@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Referencias</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Aprendizaje Profundo-Diplomado](https://github.com/AprendizajeProfundo/Diplomado)\n",
    "1. [Aprendizaje Profundo-PLN](https://github.com/AprendizajeProfundo/PLN)\n",
    "1. Ashish Vaswani et al.,  [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf), diciembre 2017.\n",
    "1. Dennis Rothman, [Transformers for Natural Language processing](http://libgen.rs/search.php?req=Transformers+for+Natural+Language+processing&open=0&res=25&view=simple&phrase=1&column=def), enero 2021.\n",
    "1. Varios,[Dive into deep learning](https://d2l.ai/), enero 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "* [Introducci贸n al concepto de atenci贸n](#Introducci贸n-al-concepto-de-atenci贸n)\n",
    "    * [Se帽ales de atenci贸n en Biolog铆a](#Se帽ales-de-atenci贸n-en-Biolog铆a)\n",
    "    * [Consultas, claves y valores](#Consultas,-claves-y-valores)\n",
    "    * [Atenci贸n agrupada. Regresi贸n kernel de Nadaraya-Watson](#Atenci贸n-agrupada.-Regresi贸n-kernel-de-Nadaraya-Watson)\n",
    "    * [Funciones de puntuaci贸n de atenci贸n](#Funciones-de-puntuaci贸n-de-atenci贸n)\n",
    "* [Mecanismos de Atenci贸n en Aprendizaje Profundo](#Mecanismos-de-Atenci贸n-en-Aprendizaje-Profundo)\n",
    "    * [Introducci贸n a mecanismos de atenci贸n](#Introducci贸n-a-mecanismos-de-atenci贸n)\n",
    "    * [Modelo seq2seq](#Modelo-seq2seq)\n",
    "    * [Atenci贸n y alineaci贸n](#Atenci贸n-y-alineaci贸n)\n",
    "    * [Modelos seq2seq y atenci贸n](#Modelos-seq2seq-y-atenci贸n)\n",
    "* [Introducci贸n a los mecanismos de auto-atenci贸n](#MIntroducci贸n-a-los-mecanismos-de-auto-atenci贸n)\n",
    "    * [Introducci贸n a auto-atenci贸n](#MIntroducci贸n-a-auto-atenci贸nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<figure>\n",
    "<img src=\"../Imagenes/logo-final-ap.png\"  width=\"80\" height=\"80\" align=\"left\"/> \n",
    "</figure>\n",
    "\n",
    "# <span style=\"color:red\"><center>Introducci贸n al concepto de atenci贸n</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Introducci贸n</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "### <center> Gracias por su atenci贸n a esta presentaci贸n. </center> \n",
    "<center> La atenci贸n es un recurso escaso</center>\n",
    "\n",
    "\n",
    "* En la era de la econom铆a de la atenci贸n, donde el cuidado humano es tratado como un bien limitado, valioso y escaso\n",
    "que se puede intercambiar, numerosos modelos de negocio se han desarrollado para capitalizarlo.\n",
    "\n",
    "\n",
    "* Al inspeccionar una escena visual, nuestro nervio 贸ptico recibe informaci贸n del orden de $ 10 ^ 8 $ bits por segundo, superando con creces lo que nuestro cerebro puede procesar por completo.\n",
    "\n",
    "* Nuestros antepasados `hab铆an aprendido de la experiencia` (tambi茅n conocidos como datos) que `no todas las entradas sensoriales son iguales`.\n",
    "\n",
    "* A lo largo de la historia de la humanidad, la capacidad de dirigir la atenci贸n solo una fracci贸n de la informaci贸n de inter茅s ha habilitado nuestro cerebro para asignar recursos de manera m谩s inteligente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Se帽ales de atenci贸n en Biolog铆a</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "\n",
    "* Estas ideas se remontan  a William James en la d茅cada de 1890.\n",
    "* Llos sujetos dirigen selectivamente el foco de atenci贸n usando tanto la `se帽al no volitiva`(no voluntaria) como la `se帽al volitiva` (voluntaria).\n",
    "\n",
    " \n",
    "* La se帽al no volitiva se basa en la prominencia y la visibilidad de los objetos en el entorno.\n",
    "\n",
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/eye-coffee.svg\" width=\"400\" height=\"400\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Auto-atenci贸n visual</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "\n",
    "Fuente [Dive into Deep learning](https://d2l.ai/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Se帽ales de atenci贸n en Biolog铆a II</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "* Despu茅s de tomar caf茅, usted est谩 cafeinado y quiere leer un libro. Entonces gira la cabeza, reenfoca sus ojos.\n",
    "* En este caso dependiente de la tarea, selecciona el libro en control cognitivo y volitivo.\n",
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/eye-book.svg\" width=\"400\" height=\"400\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Auto-atenci贸n visual</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "Fuente [Dive into Deep learning](https://d2l.ai/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Consultas, claves y valores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "* Lo que fija los mecanismos de atenci贸n  la inclusi贸n de las se帽ales volitivas. En el contexto de los mecanismos de atenci贸n, nos referimos a las se帽ales volitivas como *consultas*(`queries`).\n",
    "* Dada cualquier consulta, los mecanismos de atenci贸n sesgan la atenci贸n sobre las entradas sensoriales (por ejemplo, representaciones de caracter铆sticas intermedias) a trav茅s de `atenci贸n conjunta` (*pooling attention*).\n",
    "* Estas entradas sensoriales se denominan `valores` (*values*) en el contexto de los mecanismos de atenci贸n. \n",
    "* M谩s generalmente, cada `valor` est谩 emparejado con una `clave`, que se puede pensar en la se帽al no volitiva de esa entrada sensorial.\n",
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/qkv.svg\" width=\"360\" height=\"400\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Diagrama de atenci贸n</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente [Dive into Deep learning](https://d2l.ai/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Aprendizaje autom谩tico con mecanismos de atenci贸n</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "* El modelo de regresi贸n kernel de Nadaraya-Watson propuesto en 1964 es un ejemplo simple pero completo para demostrar el aprendizaje autom谩tico con mecanismos de atenci贸n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 3
   },
   "source": [
    "### Generaci贸n de un conjunto de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 3
   },
   "source": [
    "Para mantener las cosas simples\n",
    "consideremos el siguiente problema de regresi贸n:\n",
    "dado un conjunto de datos de pares de entrada-salida $\\{(x_1, y_1), \\ldots, (x_n, y_n)\\}$,\n",
    "como ense帽arle a $f$ a predecir la salida $\\hat{y} = f(x)$ para cualquier entranada nueva $x$?\n",
    "\n",
    "Aqu铆 generamos un conjunto de datos artificial de acuerdo con la siguiente funci贸n no lineal con el t茅rmino de ruido $\\epsilon$:\n",
    "\n",
    "$$y_i = 2\\sin(x_i) + x_i^{0.8} + \\epsilon,$$\n",
    "\n",
    "en donde $\\epsilon$ obedece a una distribuci贸n normal con media cero y desviaci贸n est谩ndar de 0,5.\n",
    "50 ejemplos de entrenamiento y 50 ejemplos de prueba\n",
    "son generadas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 9,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Agrupaci贸n promedio. Average Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 9
   },
   "source": [
    "Comenzamos con quiz谩s el estimador m谩s \"tonto\" del mundo para este problema de regresi贸n:\n",
    "utilizando la agrupaci贸n promedio para promediar todos los resultados entrenamiento:\n",
    "\n",
    "$$f(x) = \\frac{1}{n}\\sum_{i=1}^n y_i,$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/kernel_reg.png\" width=\"500\" height=\"500\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Aprendizaje autom谩tico con mecanismos de atenci贸n</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "Fuente [Dive into Deep learning](https://d2l.ai/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Atenci贸n agrupada no-param茅trica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 12
   },
   "source": [
    "Obviamente,\n",
    "la agrupaci贸n promedio omite las entradas $x_i$.\n",
    "Se propuso una mejor idea\n",
    "por Nadaraya y Watson\n",
    "para pesar las salidas $y_i$ de acuerdo con sus localizaciones de entrada:\n",
    "\n",
    "$$f(x) = \\sum_{i=1}^n \\frac{K(x - x_i)}{\\sum_{j=1}^n K(x - x_j)} y_i,$$\n",
    "\n",
    "\n",
    "en donde  $K$ es un *kernel*.\n",
    "El estimador en la ecuaci贸n anterior\n",
    "se llama *Regresi贸n de kernel de Nadaraya-Watson*. Para cualquier consulta, sus ponderaciones de atenci贸n sobre todos los pares *clave-valor* son una distribuci贸n de probabilidad v谩lida: no son negativas y suman uno.\n",
    "\n",
    "Desde la perspectiva de la atenci贸n, podemos reescribir esta ecuaci贸n en una forma de atenci贸n agrupada:\n",
    "\n",
    "$$f(x) = \\sum_{i=1}^n \\alpha(x, x_i) y_i,$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 12,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Regresi贸n Kernel Nadaraya-Watson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 12
   },
   "source": [
    "Para obtener intuiciones de la atenci贸n conjunta,\n",
    "solo considere un *kernel gaussiano* definido como $K(u) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-\\frac{u^2}{2})$. Conectando el kernel gaussiano a las ecuaciones anteriores se obtiene\n",
    "\n",
    "$$ f(x) =\\sum_{i=1}^n \\alpha(x, x_i) y_i = \\sum_{i=1}^n \\frac{\\exp\\left(-\\frac{1}{2}(x - x_i)^2\\right)}{\\sum_{j=1}^n \\exp\\left(-\\frac{1}{2}(x - x_j)^2\\right)} y_i \n",
    "$$\n",
    "\n",
    "\n",
    "En esta 煤ltima ecuaci贸n, \n",
    "una clave $x_i$ que es m谩s cercana a la consulta y $x$ prestar谩 *m谩s atenci贸n*\n",
    "via un *peso de atenci贸n m谩s grande*  para el correspondiente valor $y_i$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/kernel_reg_02.png\" width=\"380\" height=\"380\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Aprendizaje autom谩tico con mecanismos de atenci贸n</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "Fuente [Dive into Deep learning](https://d2l.ai/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Funciones de puntuaci贸n de atenci贸n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "A un nivel alto, podemos usar el algoritmo anterior para instanciar el marco de los mecanismos de atenci贸n.\n",
    "\n",
    "Denotando una funci贸n de puntuaci贸n de atenci贸n por $\\mathbf{a} $, la imagen ilustra c贸mo la salida de la atenci贸n agrupada se puede calcular como una suma ponderada de valores. Dado que los pesos de atenci贸n son una distribuci贸n de probabilidad, la suma ponderada es esencialmente un promedio ponderado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0,
    "tags": []
   },
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/attention-output.svg\" width=\"500\" height=\"500\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Funci贸n de puntaje (score)</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "Fuente [Dive into Deep learning](https://d2l.ai/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<figure>\n",
    "<img src=\"../Imagenes/logo-final-ap.png\"  width=\"80\" height=\"80\" align=\"left\"/> \n",
    "</figure>\n",
    "\n",
    "# <span style=\"color:red\"><center>Mecanismos de Atenci贸n en Aprendizaje Profundo</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Introducci贸n a mecanismos de atenci贸n</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* La maquinas de aprendizaje estad铆stico fueron durante mucho tiempo la tendencia dominate en el 谩rea de traducci贸n autom谩tica. Modernamente, esas m谩quinas han sido remplazadas por `m谩quinas neuronales de traducci贸n` (neural machine translation o NMT). la NMT est谩n basados en el framework seq2seq.\n",
    "\n",
    "* El framework seq2seq se basa en un `autoencoder`. El encoder es una red neuronal recurrente que va leyendo las palabras de entrada una por una y construye representaci贸n vectorial de cada una de estas, del mismo tama帽o (embedding). El decoder es otra red neural recurrente, que condicionada sobre las entradas va generando las palabras de salida una por una, [Sequence to Sequence Learning with Neural Networks (Sutskever et. al, 2014)](https://arxiv.org/pdf/1409.3215.pdf).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Modelo seq2seq</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* El modelo de secuencia a secuencia `seq2seq` es un modelo de aprendizaje que convierte una secuencia de entrada en una secuencia de salida. En este contexto, la secuencia es una lista de s铆mbolos, correspondiente a las palabras en una oraci贸n. \n",
    "* Todas estas tareas pueden considerarse como la tarea de aprender un modelo que convierte una secuencia de entrada en una secuencia de salida. La imagen muestra la arquitectura general del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/seq2seq_0.png\" width=\"700\" height=\"700\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Modelo cl谩sico seq2seq.</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "Fuente: [Attn: Illustrated Attention](https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3#0458)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Componentes del modelo seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo consta escencialmente de las siguientes capas: \n",
    "\n",
    "+ capa de incrustaci贸n de codificador,  capa recurrente del codificador, capa de incrustaci贸n de decodificador, capa recurrente del decodificador, capa de salida del decodificador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/seq2seq.png\" width=\"600\" height=\"500\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Estructura autoencoder de un modelo seq2seq de *pregunta/respuesta*.</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Write a Sequence to Sequence (seq2seq) Model](https://docs.chainer.org/en/stable/examples/seq2seq.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Atenci贸n y alineaci贸n </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Atenci贸n es un mecanismo que se introduce entre el codificador y el decodificador. El decodificador sigue recibiendo el estado oculto final de la secuencia de entrada como antes, mientras que el mecanismo de atenci贸n se basa en las predicciones intermedias en el estado oculto.\n",
    "* En este contexto, alineaci贸n significa hacer coincidir segmentos de la entrada con segmento de la traducci贸n, [Googles Neural Machine Translation System: Bridging the Gap between Human and Machine Translation (Wu et. al, 2016)](https://arxiv.org/pdf/1609.08144.pdf). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/mecansimo_atencion.png\" width=\"450\" height=\"500\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Modelo conceptual de atenci贸n.</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Write a Sequence to Sequence (seq2seq) Model](https://docs.chainer.org/en/stable/examples/seq2seq.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Modelos seq2seq y atenci贸n</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El mecanismo de atenci贸n se enfoca sobre diferentes palabras asignando a cada palabra un puntaje (`score`). Estos puntajes son transformados con la funci贸n `softmax`, lo cual asigna un peso cada palabra. Con estos puntajes los estados ocultos del encoder son agregados mediante una suma pesada de los estados ocultos usando tales pesos. La construcci贸n de una capa atencional puede subdividirse en cuatro pasos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Paso 0. Preparaci贸n de los estados ocultos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero se preparan todos los estados ocultos del encoder y el primer estado oculto del decoder. En la imagen los estos ocultos del encoder son los c铆rculos en verde y el primer estado oculto del decoder se muestra en color rojo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Paso_0_Atencion.gif\" width=\"500\" height=\"600\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Modelo de atenci贸n. Paso 0. preparaci贸n de los estados ocultos</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Write a Sequence to Sequence (seq2seq) Model](https://docs.chainer.org/en/stable/examples/seq2seq.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Paso 1. Obtener un puntaje para cada estado oculto del encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El puntaje asociado para cada estado oculto del encoder se obtiene mediante el producto escalar entre el valor del estado oculto y el valor del primer estado oculto del decoder. Si $hd_1 = (y_{11},\\ldots, y_{1m})$ es el vector que representa el primer estado oculto del decoder y $he_i = (x_{i1},\\ldots, x_{im})$ representa el i-茅simo estado oculto del encoder, entoces el puntaje asociado al estado oculto $i$ es dado por\n",
    "\n",
    "$$\n",
    "\\text{score}_{i1} =<hd_1, he_i> = y_{11}x_{i1}+ \\ldots + y_{1m}x_{im}, i =1,\\cdots, m\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Paso_1_Atencion.gif\" width=\"500\" height=\"500\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Model de atenci贸n. Paso 1. Puntajes para cada estado oculto del encoder</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Write a Sequence to Sequence (seq2seq) Model](https://docs.chainer.org/en/stable/examples/seq2seq.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Paso 2. Transformar todos los puntajes mediante la funci贸n softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurede que con esta transfromaci贸n todos los puntajes quedan en el intervalo $[0, 1]$ y suman 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Paso_2_Atencion.gif\" width=\"500\" height=\"500\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Modelo de atenci贸n. Paso 2. Los puntajes para cada estado oculto del encoder se transforman a la escala softmax</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Write a Sequence to Sequence (seq2seq) Model](https://docs.chainer.org/en/stable/examples/seq2seq.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Paso 3. Multiplica cada estado oculto del encoder por su puntaje softmax: vectores de alineaci贸n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "he_i \\leftarrow score_i * he_i , i =1,\\cdots, m\n",
    "$$\n",
    "\n",
    "El resultado se llama vector de alineaci贸n  (`alignment vector`)  o vector de anotaci贸n (`annotation vector`). Este el momento ex谩cto en el cual ocurre la alineaci贸n entre la entrada y la salida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Paso_3_Atencion.gif\" width=\"500\" height=\"600\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Modelo de atenci贸n. Paso 3. Vectores de alineaci贸n (softmax)</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Write a Sequence to Sequence (seq2seq) Model](https://docs.chainer.org/en/stable/examples/seq2seq.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Paso 4. Suma los vectores de alineaci贸n: vector de contexto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|encoder| puntaje | softmax| alineaci贸n|\n",
    "|---|---|---|---|\n",
    "|[0, 2, 5] | 13 | 0 | [0, 0, 0]|\n",
    "|[3, 5, 4] | 30 | 1 | [3, 5, 4]|\n",
    "|[2, 1, 0] | 5 | 0 | [0, 0, 0]|\n",
    "|[1, 1, 0] | 6 | 0 | [0, 0, 0]|\n",
    "**context** = [3, 5, 4] = [0, 0, 0] + [3, 5, 4] +  [0, 0, 0] + [0, 0, 0] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Paso_4_Atencion.gif\" width=\"500\" height=\"500\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Modelo de atenci贸n. Paso 4. vector de contexto</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Write a Sequence to Sequence (seq2seq) Model](https://docs.chainer.org/en/stable/examples/seq2seq.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Paso 5. Alimenta el decodificador con el vector de contexo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La forma como se haga esto depende de la arquitectura de dise帽o. Vamos a ver algunas de estas arquitecturas en las siguientes secciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Paso_5_Atencion.gif\" width=\"500\" height=\"500\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Modelo de atenci贸n. Paso 5. Alimenta el decodificador con el vector de contexto</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Write a Sequence to Sequence (seq2seq) Model](https://docs.chainer.org/en/stable/examples/seq2seq.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "La siguiente imagen muestra el proceso completo en la capa de atenci贸n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Atencion_Ilustrado_1.png\" width=\"500\" height=\"500\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Capa de atenci贸n completa</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Write a Sequence to Sequence (seq2seq) Model](https://docs.chainer.org/en/stable/examples/seq2seq.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Ejemplo: Traducci贸n autom谩tica neuronal de Google (GNMT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Googles Neural Machine Translation System: Bridging the Gap between Human and Machine Translation (Wu et. al, 2016)](https://arxiv.org/pdf/1609.08144.pdf). Debido a que la mayor铆a de nosotros debe haber usado Google Translate de una forma u otra, parece imperativo hablar sobre el NMT de Google, que se implement贸 en 2016. GNMT es una combinaci贸n de los 2 ejemplos anteriores que hemos visto (muy inspirados en el primero).\n",
    "\n",
    "1. El codificador consta de una pila de 8 LSTM, donde el primero es bidireccional (cuyas salidas est谩n concatenados), y existe una conexi贸n residual entre las salidas de capas consecutivas (a partir de la 3陋 capa). El decodificador es una pila separada de 8 LSTM unidireccionales.\n",
    "1. La funci贸n de puntuaci贸n utilizada es el aditivo / concat, como en [1].\n",
    "1. Nuevamente, como en [Neural Machine Translation by Jointly Learning to Align and Translate (Bahdanau et. al, 2015)](https://arxiv.org/pdf/1409.0473.pdf), la entrada al siguiente paso del decodificador es la concatenaci贸n entre la salida del paso de tiempo del decodificador anterior (rosa) y el vector de contexto del paso de tiempo actual (verde oscuro)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Traducci贸n autom谩tica neuronal de Google"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/NMT_Google.png\" width=\"250\" height=\"500\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">NMT. Google. Las conexiones de salto se indican mediante flechas curvas. Tenga en cuenta que las celdas LSTM solo muestran el estado oculto y la entrada; no muestra la entrada del estado de la celda.</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Write a Sequence to Sequence (seq2seq) Model](https://docs.chainer.org/en/stable/examples/seq2seq.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<figure>\n",
    "<img src=\"../Imagenes/logo-final-ap.png\"  width=\"80\" height=\"80\" align=\"left\"/> \n",
    "</figure>\n",
    "\n",
    "# <span style=\"color:red\"><center>Introducci贸n a los mecanismos de auto-atenci贸n</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Introducci贸n a auto-atenci贸n</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "驴Qu茅 tienen en com煤n BERT, RoBERTa, ALBERT, SpanBERT, DistilBERT, SesameBERT, SemBERT, SciBERT, BioBERT, MobileBERT, TinyBERT y CamemBERT?. NO es propiamente BERT.\n",
    "\n",
    "Respuesta: auto-atenci贸n (`self-attention`).  El modelado moderno de tareas de comprensi贸n del lenguaje conf铆an por completo en los mecanismos de auto-atenci贸n para generar dependencias globales entre entradas y salidas. \n",
    "\n",
    "\n",
    "La auto-atenci贸n similar a la atenci贸n.  Un mecanismo de atenci贸n siguen el siguiente algoritmo.\n",
    "\n",
    "1. Preparar entradas\n",
    "1. Inicializar los pesos\n",
    "1. Obtener `clave` (key), `consulta` (query) y `valor` (value).\n",
    "1. Calcule las puntuaciones de atenci贸n para la entrada 1.\n",
    "1. Calcular softmax de las puntuaciones.\n",
    "1. Multiplica las puntuaciones softmax con los `valores`\n",
    "1. Sumar `valores ponderados` para obtener la Salida 1\n",
    "1. Repite los pasos 4 a 7 para la entrada 2 y la entrada 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Paso 1. Preparar las entradas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las entradas correponden a una secuencia de palabras sumergidas (embebidas) en un sumergimiento (embbeding) que vamos a supone que es de tama帽o 4. Para el ejemplo supondremos que que la secuencia consta de 3 palabras. Entonces tenemos 3 entradas de tama帽a 4.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/self_attention_1.gif\" width=\"700\" height=\"700\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Auto-atenci贸n: Entradas.</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "Fuente: [Illustrated self attention](https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el ejemplo tenemos tres entradas.\n",
    "\n",
    "1. entrada 1: $e_1=[1, 0, 1, 0]$\n",
    "1. entrada 2: $e_2=[0, 2, 0, 2]$\n",
    "1. entrada 3: $e_3=[1, 1, 1, 1]$\n",
    "\n",
    "Matricialmente escribimos\n",
    "\n",
    "$$\n",
    "E = \\begin{pmatrix}\n",
    "1 & 0 & 1 & 0\\\\\n",
    "0 & 2 & 0 & 2\\\\\n",
    "1 & 1 & 1 &1\n",
    "\\end{pmatrix}= \\begin{pmatrix} \n",
    "e_1\\\\\n",
    "e_2\\\\\n",
    "e_3\\\\\n",
    "\\end{pmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Paso 2. Inicializar los pesos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada una de las entradas se transforman en tres diferentes vectores de alg煤n tama帽o. Para ello se cosntruyen tres transfomaciones lineales (o afines), digamos $W_k$, $W_q$ y $W_v$. Esta matrices son par谩metros de la red neuronal que son inicializados aleatoriamente, o con alg煤n procedimiento est谩ndar de inicializaci贸n de pesos.\n",
    "\n",
    "Para esta ilustraci贸n, supondremos que las matrices de pesos son dadas por\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "W_k = \n",
    "\\begin{pmatrix} \n",
    "0 & 0 &1\\\\\n",
    "1 & 1 &0\\\\\n",
    "0 & 1 &0\\\\\n",
    "1 & 1 &0\n",
    "\\end{pmatrix}, \\quad\n",
    "W_v = \n",
    "\\begin{pmatrix} \n",
    "0 & 2 &0\\\\\n",
    "0 & 3 &0\\\\\n",
    "1 & 0 &3\\\\\n",
    "1 & 1 &0\n",
    "\\end{pmatrix}, \\quad\n",
    "W_q = \n",
    "\\begin{pmatrix} \n",
    "1 & 0 &1\\\\\n",
    "1 & 0 &0\\\\\n",
    "0 & 0 &1\\\\\n",
    "0 & 1 &1\n",
    "\\end{pmatrix}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Paso 3. Obtener claves, consultas y valores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este paso obtenemos los tres tipos de objetos derivados de la entrada: claves, consultas y valores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Claves (keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "K = E \\times W_k = \n",
    "\\begin{pmatrix} \n",
    "1 & 0 &1 &0\\\\\n",
    "0 & 2 &0 &2\\\\\n",
    "1 & 1 &1 &1\\\\\n",
    "\\end{pmatrix} \\times\n",
    "\\begin{pmatrix} \n",
    "0 & 0 &1\\\\\n",
    "1 & 1 &0\\\\\n",
    "0 & 1 &0\\\\\n",
    "1 & 1 &0\n",
    "\\end{pmatrix}  =\n",
    "\\begin{pmatrix} \n",
    "0 & 1 &1\\\\\n",
    "4 & 4 &0 \\\\\n",
    "2 & 3 &1 \\\\\n",
    "\\end{pmatrix}=\n",
    "\\begin{pmatrix} \n",
    "k_1\\\\\n",
    "k_2\\\\\n",
    "k_3\\\\\n",
    "\\end{pmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Valores (values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "V = E \\times W_v = \n",
    "\\begin{pmatrix} \n",
    "1 & 0 &1 &0\\\\\n",
    "0 & 2 &0 &2\\\\\n",
    "1 & 1 &1 &1\\\\\n",
    "\\end{pmatrix} \\times\n",
    "\\begin{pmatrix} \n",
    "0 & 2 &0\\\\\n",
    "0 & 3 &0\\\\\n",
    "1 & 0 &3\\\\\n",
    "1 & 1 &0\n",
    "\\end{pmatrix}   =\n",
    "\\begin{pmatrix} \n",
    "1 & 2 &3\\\\\n",
    "2 & 8 &0 \\\\\n",
    "2 & 6 &3 \\\\\n",
    "\\end{pmatrix}=\n",
    "\\begin{pmatrix} \n",
    "v_1\\\\\n",
    "v_2\\\\\n",
    "v_3\\\\\n",
    "\\end{pmatrix}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consultas (queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "Q = E \\times W_q = \n",
    "\\begin{pmatrix} \n",
    "1 & 0 &1 &0\\\\\n",
    "0 & 2 &0 &2\\\\\n",
    "1 & 1 &1 &1\\\\\n",
    "\\end{pmatrix} \\times\n",
    "\\begin{pmatrix} \n",
    "1 & 0 &1\\\\\n",
    "1 & 0 &0\\\\\n",
    "0 & 0 &1\\\\\n",
    "0 & 1 &1\n",
    "\\end{pmatrix}   =\n",
    "\\begin{pmatrix} \n",
    "1 & 0 &2\\\\\n",
    "2 & 2 &2 \\\\\n",
    "2 & 1 &3 \\\\\n",
    "\\end{pmatrix}=\n",
    "\\begin{pmatrix} \n",
    "q_1\\\\\n",
    "q_2\\\\\n",
    "q_3\\\\\n",
    "\\end{pmatrix}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/self_attention_2.gif\" width=\"800\" height=\"800\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Auto-atenci贸n: Obtenci贸n de claves consultas y valores.</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Illustrated self attention](https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Paso 4. C谩lculo de los puntajes de auto-atenci贸n para las entradas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ilustramos aqu铆 como se calculan los puntajes de auto-atenci贸n para la entrada 1. Los puntajes de auto-atenci贸n para las dem谩s entradas se calculan de la misma manera con el cambio obvio.\n",
    "\n",
    "\n",
    "Como se muestra en la siguiente ilustraci贸n, se toma la consulta obtenida para la primera entrada, que en el ejemplo es $e_1 =[1, 0, 2]$. Los pesos de auto-atenci贸n son una medida de similaridad, entre la consulta y cada una de las claves, como se ha estudiado en las lecciones atenci贸n. La funci贸n de auto-atenci贸n se denotar谩 $a$ . En este caso el resultado se obtiene haciendo el producto escalar (producto punto) entre la consulta asociada  a $e_1$, es decir, $q_1$ y cada una de las claves, es decir $a(q_1, c_i)= <q_1, c_i>, i =1,2,3$.\n",
    "\n",
    "Escrito en forma matricial, los pesos de auto-atenci贸n para la entrada 1 se calcula como\n",
    "\n",
    "\n",
    "$$\n",
    "p_1 = q_1 \\times K^T = [1, 0, 2]\\begin{pmatrix} \n",
    "0 & 4 & 2 \\\\\n",
    "1 & 4 & 3 \\\\\n",
    "1 & 0 & 1 \\\\\n",
    "\\end{pmatrix} =[2, 4, 4].\n",
    "$$\n",
    "\n",
    "Los pesos de auto-atenci贸n completos se calculan mediante\n",
    "\n",
    "$$\n",
    "P = Q \\times  K^T =\n",
    "\\begin{pmatrix} \n",
    "1 & 0 &2\\\\\n",
    "2 & 2 &2 \\\\\n",
    "2 & 1 &3 \\\\\n",
    "\\end{pmatrix} \\times\n",
    "\\begin{pmatrix} \n",
    "0 & 4 & 2 \\\\\n",
    "1 & 4 & 3 \\\\\n",
    "1 & 0 & 1 \\\\\n",
    "\\end{pmatrix}= \\begin{pmatrix} \n",
    "2 & 4 & 4 \\\\\n",
    "4 & 16 & 12 \\\\\n",
    "4 & 12 & 10 \\\\\n",
    "\\end{pmatrix} = \\begin{pmatrix} \n",
    "p_1\\\\\n",
    "p_2\\\\\n",
    "p_3\\\\\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/self_attention_3.gif\" width=\"800\" height=\"800\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Auto-atenci贸n: Obtenci贸n de puntajes de auto-atenci贸n.</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Illustrated self attention](https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 5. C谩lculo del puntaje softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformamos los puntajes a la escala softmax. Por ejemplo softmax([2, 4, 4]) = [0.06, 0.47, 0.47]. Observe que softmax define una distribuci贸n discreta de probabilidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Paso 6. Multiplica los puntajes softmax con los valores para cada consulta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "S = \\begin{pmatrix}\n",
    "\\text{softmax}(p_1)\\\\\n",
    "\\text{softmax}(p_2)\\\\\n",
    "\\text{softmax}(p_3)\n",
    "\\end{pmatrix} = \n",
    "\\begin{pmatrix} \n",
    "0.06 & 0.47 &0.47\\\\\n",
    "0.00 & 0.98 &0.02 \\\\\n",
    "0.00 & 0.88 &0.12 \\\\\n",
    "\\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    "s_1\\\\\n",
    "s_2\\\\\n",
    "s_3\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la consulta 1 se tiene\n",
    "* 0.06 * [1, 2, 3] = [0.06, 0.13, 0.19]\n",
    "* 0.47 * [2, 8, 0] = [0.94, 3.75, 0.00]\n",
    "* 0.47 * [2, 6, 3] = [0.94, 2.81, 1.40]\n",
    "\n",
    "Para la consulta 2\n",
    "* 0.00 * [1, 2, 3] = [0.00, 0.00, 0.00]\n",
    "* 0.98 * [2, 8, 0] = [1.96, 7.86, 0.00]\n",
    "* 0.02 * [2, 6, 3] = [0.04, 0.11, 0.05]\n",
    "\n",
    "Para la consulta 3\n",
    "* 0.00 * [1, 2, 3] = [0.00, 0.00, 0.00]\n",
    "* 0.88 * [2, 8, 0] = [1.76, 7.04, 0.00]\n",
    "* 0.12 * [2, 6, 3] = [0.24, 0.72, 0.36]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Paso 7. Suma pesada de valores para conseguir las salidas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada sal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada salida se obtiene como la suma de los vectores pesados asociados a cada consulta as铆:\n",
    "\n",
    "* salida 1: [0.06, 0.13, 0.19] + [0.94, 3.75, 0.00] + [0.94, 2.81, 1.40] = [1.94, 6.68, 1.60]\n",
    "* salida 2: [0.00, 0.00, 0.00] + [1.96, 7.86, 0.00] + [0.04, 0.11, 0.05] = [2.00, 7.96, 0.05]\n",
    "* salida 3: [0.00, 0.00, 0.00] + [1.76, 7.04, 0.00] + [0.24, 0.72, 0.36] = [2.00, 7.76, 0.36]\n",
    "\n",
    "El siguiente gif muestra el modelo completo de auto-atenci贸n. Los valores se redondearon. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/self_attention_4.gif\" width=\"800\" height=\"800\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Auto-atenci贸n: C谩lculo completo de las 煤ltimas dos salidas</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Illustrated self attention](https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
