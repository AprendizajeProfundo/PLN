{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<figure>\n",
    "<img src=\"../Imagenes/logo-final-ap.png\"  width=\"80\" height=\"80\" align=\"left\"/> \n",
    "</figure>\n",
    "\n",
    "# <span style=\"color:blue\"><left>Aprendizaje Profundo</left></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"><center>Atención y auto-atención</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<center>All you need is attention!</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Profesores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co\n",
    "2. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com \n",
    "3. Campo Elías Pardo Turriago, cepardot@unal.edu.co "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Asesora Medios y Marketing digital</span>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Maria del Pilar Montenegro, pmontenegro88@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Referencias</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Aprendizaje Profundo-Diplomado](https://github.com/AprendizajeProfundo/Diplomado)\n",
    "1. [Aprendizaje Profundo-PLN](https://github.com/AprendizajeProfundo/PLN)\n",
    "1. Ashish Vaswani et al.,  [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf), diciembre 2017.\n",
    "1. Dennis Rothman, [Transformers for Natural Language processing](http://libgen.rs/search.php?req=Transformers+for+Natural+Language+processing&open=0&res=25&view=simple&phrase=1&column=def), enero 2021.\n",
    "1. Varios,[Dive into deep learning](https://d2l.ai/), enero 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "* [Introducción al concepto de atención](#Introducción-al-concepto-de-atención)\n",
    "    * [Señales de atención en Biología](#Señales-de-atención-en-Biología)\n",
    "    * [Consultas, claves y valores](#Consultas,-claves-y-valores)\n",
    "    * [Atención agrupada. Regresión kernel de Nadaraya-Watson](#Atención-agrupada.-Regresión-kernel-de-Nadaraya-Watson)\n",
    "    * [Funciones de puntuación de atención](#Funciones-de-puntuación-de-atención)\n",
    "* [Mecanismos de Atención en Aprendizaje Profundo](#Mecanismos-de-Atención-en-Aprendizaje-Profundo)\n",
    "    * [Introducción a mecanismos de atención](#Introducción-a-mecanismos-de-atención)\n",
    "    * [Modelo seq2seq](#Modelo-seq2seq)\n",
    "    * [Atención y alineación](#Atención-y-alineación)\n",
    "    * [Modelos seq2seq y atención](#Modelos-seq2seq-y-atención)\n",
    "* [Introducción a los mecanismos de auto-atención](#MIntroducción-a-los-mecanismos-de-auto-atención)\n",
    "    * [Introducción a auto-atención](#MIntroducción-a-auto-atenciónn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<figure>\n",
    "<img src=\"../Imagenes/logo-final-ap.png\"  width=\"80\" height=\"80\" align=\"left\"/> \n",
    "</figure>\n",
    "\n",
    "# <span style=\"color:red\"><center>Introducción al concepto de atención</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "### <center> Gracias por su atención a esta presentación. </center> \n",
    "<center> La atención es un recurso escaso</center>\n",
    "\n",
    "\n",
    "* En la era de la economía de la atención, donde el cuidado humano es tratado como un bien limitado, valioso y escaso\n",
    "que se puede intercambiar, numerosos modelos de negocio se han desarrollado para capitalizarlo.\n",
    "\n",
    "\n",
    "* Al inspeccionar una escena visual, nuestro nervio óptico recibe información del orden de $ 10 ^ 8 $ bits por segundo, superando con creces lo que nuestro cerebro puede procesar por completo.\n",
    "\n",
    "* Nuestros antepasados `habían aprendido de la experiencia` (también conocidos como datos) que `no todas las entradas sensoriales son iguales`.\n",
    "\n",
    "* A lo largo de la historia de la humanidad, la capacidad de dirigir la atención solo una fracción de la información de interés ha habilitado nuestro cerebro para asignar recursos de manera más inteligente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Señales de atención en Biología</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "\n",
    "* Estas ideas se remontan  a William James en la década de 1890.\n",
    "* Llos sujetos dirigen selectivamente el foco de atención usando tanto la `señal no volitiva`(no voluntaria) como la `señal volitiva` (voluntaria).\n",
    "\n",
    " \n",
    "* La señal no volitiva se basa en la prominencia y la visibilidad de los objetos en el entorno.\n",
    "\n",
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/eye-coffee.svg\" width=\"400\" height=\"400\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Auto-atención visual</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "\n",
    "Fuente [Dive into Deep learning](https://d2l.ai/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Señales de atención en Biología II</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "* Después de tomar café, usted está cafeinado y quiere leer un libro. Entonces gira la cabeza, reenfoca sus ojos.\n",
    "* En este caso dependiente de la tarea, selecciona el libro en control cognitivo y volitivo.\n",
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/eye-book.svg\" width=\"400\" height=\"400\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Auto-atención visual</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "Fuente [Dive into Deep learning](https://d2l.ai/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Consultas, claves y valores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "* Lo que fija los mecanismos de atención  la inclusión de las señales volitivas. En el contexto de los mecanismos de atención, nos referimos a las señales volitivas como *consultas*(`queries`).\n",
    "* Dada cualquier consulta, los mecanismos de atención sesgan la atención sobre las entradas sensoriales (por ejemplo, representaciones de características intermedias) a través de `atención conjunta` (*pooling attention*).\n",
    "* Estas entradas sensoriales se denominan `valores` (*values*) en el contexto de los mecanismos de atención. \n",
    "* Más generalmente, cada `valor` está emparejado con una `clave`, que se puede pensar en la señal no volitiva de esa entrada sensorial.\n",
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/qkv.svg\" width=\"360\" height=\"400\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Diagrama de atención</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente [Dive into Deep learning](https://d2l.ai/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Aprendizaje automático con mecanismos de atención</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "* El modelo de regresión kernel de Nadaraya-Watson propuesto en 1964 es un ejemplo simple pero completo para demostrar el aprendizaje automático con mecanismos de atención."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 3
   },
   "source": [
    "### Generación de un conjunto de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 3
   },
   "source": [
    "Para mantener las cosas simples\n",
    "consideremos el siguiente problema de regresión:\n",
    "dado un conjunto de datos de pares de entrada-salida $\\{(x_1, y_1), \\ldots, (x_n, y_n)\\}$,\n",
    "como enseñarle a $f$ a predecir la salida $\\hat{y} = f(x)$ para cualquier entranada nueva $x$?\n",
    "\n",
    "Aquí generamos un conjunto de datos artificial de acuerdo con la siguiente función no lineal con el término de ruido $\\epsilon$:\n",
    "\n",
    "$$y_i = 2\\sin(x_i) + x_i^{0.8} + \\epsilon,$$\n",
    "\n",
    "en donde $\\epsilon$ obedece a una distribución normal con media cero y desviación estándar de 0,5.\n",
    "50 ejemplos de entrenamiento y 50 ejemplos de prueba\n",
    "son generadas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 9,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Agrupación promedio. Average Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 9
   },
   "source": [
    "Comenzamos con quizás el estimador más \"tonto\" del mundo para este problema de regresión:\n",
    "utilizando la agrupación promedio para promediar todos los resultados entrenamiento:\n",
    "\n",
    "$$f(x) = \\frac{1}{n}\\sum_{i=1}^n y_i,$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/kernel_reg.png\" width=\"500\" height=\"500\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Aprendizaje automático con mecanismos de atención</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "Fuente [Dive into Deep learning](https://d2l.ai/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Atención agrupada no-paramétrica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 12
   },
   "source": [
    "Obviamente,\n",
    "la agrupación promedio omite las entradas $x_i$.\n",
    "Se propuso una mejor idea\n",
    "por Nadaraya y Watson\n",
    "para pesar las salidas $y_i$ de acuerdo con sus localizaciones de entrada:\n",
    "\n",
    "$$f(x) = \\sum_{i=1}^n \\frac{K(x - x_i)}{\\sum_{j=1}^n K(x - x_j)} y_i,$$\n",
    "\n",
    "\n",
    "en donde  $K$ es un *kernel*.\n",
    "El estimador en la ecuación anterior\n",
    "se llama *Regresión de kernel de Nadaraya-Watson*. Para cualquier consulta, sus ponderaciones de atención sobre todos los pares *clave-valor* son una distribución de probabilidad válida: no son negativas y suman uno.\n",
    "\n",
    "Desde la perspectiva de la atención, podemos reescribir esta ecuación en una forma de atención agrupada:\n",
    "\n",
    "$$f(x) = \\sum_{i=1}^n \\alpha(x, x_i) y_i,$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 12,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Regresión Kernel Nadaraya-Watson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 12
   },
   "source": [
    "Para obtener intuiciones de la atención conjunta,\n",
    "solo considere un *kernel gaussiano* definido como $K(u) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-\\frac{u^2}{2})$. Conectando el kernel gaussiano a las ecuaciones anteriores se obtiene\n",
    "\n",
    "$$ f(x) =\\sum_{i=1}^n \\alpha(x, x_i) y_i = \\sum_{i=1}^n \\frac{\\exp\\left(-\\frac{1}{2}(x - x_i)^2\\right)}{\\sum_{j=1}^n \\exp\\left(-\\frac{1}{2}(x - x_j)^2\\right)} y_i \n",
    "$$\n",
    "\n",
    "\n",
    "En esta última ecuación, \n",
    "una clave $x_i$ que es más cercana a la consulta y $x$ prestará *más atención*\n",
    "via un *peso de atención más grande*  para el correspondiente valor $y_i$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/kernel_reg_02.png\" width=\"380\" height=\"380\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Aprendizaje automático con mecanismos de atención</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "Fuente [Dive into Deep learning](https://d2l.ai/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Funciones de puntuación de atención"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "A un nivel alto, podemos usar el algoritmo anterior para instanciar el marco de los mecanismos de atención.\n",
    "\n",
    "Denotando una función de puntuación de atención por $\\mathbf{a} $, la imagen ilustra cómo la salida de la atención agrupada se puede calcular como una suma ponderada de valores. Dado que los pesos de atención son una distribución de probabilidad, la suma ponderada es esencialmente un promedio ponderado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0,
    "tags": []
   },
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/attention-output.svg\" width=\"500\" height=\"500\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Función de puntaje (score)</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "Fuente [Dive into Deep learning](https://d2l.ai/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<figure>\n",
    "<img src=\"../Imagenes/logo-final-ap.png\"  width=\"80\" height=\"80\" align=\"left\"/> \n",
    "</figure>\n",
    "\n",
    "# <span style=\"color:red\"><center>Mecanismos de Atención en Aprendizaje Profundo</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Introducción a mecanismos de atención</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* La maquinas de aprendizaje estadístico fueron durante mucho tiempo la tendencia dominate en el área de traducción automática. Modernamente, esas máquinas han sido remplazadas por `máquinas neuronales de traducción` (neural machine translation o NMT). la NMT están basados en el framework seq2seq.\n",
    "\n",
    "* El framework seq2seq se basa en un `autoencoder`. El encoder es una red neuronal recurrente que va leyendo las palabras de entrada una por una y construye representación vectorial de cada una de estas, del mismo tamaño (embedding). El decoder es otra red neural recurrente, que condicionada sobre las entradas va generando las palabras de salida una por una, [Sequence to Sequence Learning with Neural Networks (Sutskever et. al, 2014)](https://arxiv.org/pdf/1409.3215.pdf).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Modelo seq2seq</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* El modelo de secuencia a secuencia `seq2seq` es un modelo de aprendizaje que convierte una secuencia de entrada en una secuencia de salida. En este contexto, la secuencia es una lista de símbolos, correspondiente a las palabras en una oración. \n",
    "* Todas estas tareas pueden considerarse como la tarea de aprender un modelo que convierte una secuencia de entrada en una secuencia de salida. La imagen muestra la arquitectura general del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/seq2seq_0.png\" width=\"700\" height=\"700\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Modelo clásico seq2seq.</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "Fuente: [Attn: Illustrated Attention](https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3#0458)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Componentes del modelo seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo consta escencialmente de las siguientes capas: \n",
    "\n",
    "+ capa de incrustación de codificador,  capa recurrente del codificador, capa de incrustación de decodificador, capa recurrente del decodificador, capa de salida del decodificador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/seq2seq.png\" width=\"600\" height=\"500\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Estructura autoencoder de un modelo seq2seq de *pregunta/respuesta*.</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Write a Sequence to Sequence (seq2seq) Model](https://docs.chainer.org/en/stable/examples/seq2seq.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Atención y alineación </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Atención es un mecanismo que se introduce entre el codificador y el decodificador. El decodificador sigue recibiendo el estado oculto final de la secuencia de entrada como antes, mientras que el mecanismo de atención se basa en las predicciones intermedias en el estado oculto.\n",
    "* En este contexto, alineación significa hacer coincidir segmentos de la entrada con segmento de la traducción, [Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation (Wu et. al, 2016)](https://arxiv.org/pdf/1609.08144.pdf). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/mecansimo_atencion.png\" width=\"450\" height=\"500\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Modelo conceptual de atención.</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Write a Sequence to Sequence (seq2seq) Model](https://docs.chainer.org/en/stable/examples/seq2seq.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Modelos seq2seq y atención</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El mecanismo de atención se enfoca sobre diferentes palabras asignando a cada palabra un puntaje (`score`). Estos puntajes son transformados con la función `softmax`, lo cual asigna un peso cada palabra. Con estos puntajes los estados ocultos del encoder son agregados mediante una suma pesada de los estados ocultos usando tales pesos. La construcción de una capa atencional puede subdividirse en cuatro pasos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Paso 0. Preparación de los estados ocultos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero se preparan todos los estados ocultos del encoder y el primer estado oculto del decoder. En la imagen los estos ocultos del encoder son los círculos en verde y el primer estado oculto del decoder se muestra en color rojo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Paso_0_Atencion.gif\" width=\"500\" height=\"600\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Modelo de atención. Paso 0. preparación de los estados ocultos</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Write a Sequence to Sequence (seq2seq) Model](https://docs.chainer.org/en/stable/examples/seq2seq.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Paso 1. Obtener un puntaje para cada estado oculto del encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El puntaje asociado para cada estado oculto del encoder se obtiene mediante el producto escalar entre el valor del estado oculto y el valor del primer estado oculto del decoder. Si $hd_1 = (y_{11},\\ldots, y_{1m})$ es el vector que representa el primer estado oculto del decoder y $he_i = (x_{i1},\\ldots, x_{im})$ representa el i-ésimo estado oculto del encoder, entoces el puntaje asociado al estado oculto $i$ es dado por\n",
    "\n",
    "$$\n",
    "\\text{score}_{i1} =<hd_1, he_i> = y_{11}x_{i1}+ \\ldots + y_{1m}x_{im}, i =1,\\cdots, m\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Paso_1_Atencion.gif\" width=\"500\" height=\"500\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Model de atención. Paso 1. Puntajes para cada estado oculto del encoder</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Write a Sequence to Sequence (seq2seq) Model](https://docs.chainer.org/en/stable/examples/seq2seq.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Paso 2. Transformar todos los puntajes mediante la función softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurede que con esta transfromación todos los puntajes quedan en el intervalo $[0, 1]$ y suman 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Paso_2_Atencion.gif\" width=\"500\" height=\"500\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Modelo de atención. Paso 2. Los puntajes para cada estado oculto del encoder se transforman a la escala softmax</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Write a Sequence to Sequence (seq2seq) Model](https://docs.chainer.org/en/stable/examples/seq2seq.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Paso 3. Multiplica cada estado oculto del encoder por su puntaje softmax: vectores de alineación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "he_i \\leftarrow score_i * he_i , i =1,\\cdots, m\n",
    "$$\n",
    "\n",
    "El resultado se llama vector de alineación  (`alignment vector`)  o vector de anotación (`annotation vector`). Este el momento exácto en el cual ocurre la alineación entre la entrada y la salida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Paso_3_Atencion.gif\" width=\"500\" height=\"600\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Modelo de atención. Paso 3. Vectores de alineación (softmax)</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Write a Sequence to Sequence (seq2seq) Model](https://docs.chainer.org/en/stable/examples/seq2seq.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Paso 4. Suma los vectores de alineación: vector de contexto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|encoder| puntaje | softmax| alineación|\n",
    "|---|---|---|---|\n",
    "|[0, 2, 5] | 13 | 0 | [0, 0, 0]|\n",
    "|[3, 5, 4] | 30 | 1 | [3, 5, 4]|\n",
    "|[2, 1, 0] | 5 | 0 | [0, 0, 0]|\n",
    "|[1, 1, 0] | 6 | 0 | [0, 0, 0]|\n",
    "**context** = [3, 5, 4] = [0, 0, 0] + [3, 5, 4] +  [0, 0, 0] + [0, 0, 0] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Paso_4_Atencion.gif\" width=\"500\" height=\"500\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Modelo de atención. Paso 4. vector de contexto</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Write a Sequence to Sequence (seq2seq) Model](https://docs.chainer.org/en/stable/examples/seq2seq.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Paso 5. Alimenta el decodificador con el vector de contexo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La forma como se haga esto depende de la arquitectura de diseño. Vamos a ver algunas de estas arquitecturas en las siguientes secciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Paso_5_Atencion.gif\" width=\"500\" height=\"500\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Modelo de atención. Paso 5. Alimenta el decodificador con el vector de contexto</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Write a Sequence to Sequence (seq2seq) Model](https://docs.chainer.org/en/stable/examples/seq2seq.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "La siguiente imagen muestra el proceso completo en la capa de atención."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Atencion_Ilustrado_1.png\" width=\"500\" height=\"500\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Capa de atención completa</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Write a Sequence to Sequence (seq2seq) Model](https://docs.chainer.org/en/stable/examples/seq2seq.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Ejemplo: Traducción automática neuronal de Google (GNMT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation (Wu et. al, 2016)](https://arxiv.org/pdf/1609.08144.pdf). Debido a que la mayoría de nosotros debe haber usado Google Translate de una forma u otra, parece imperativo hablar sobre el NMT de Google, que se implementó en 2016. GNMT es una combinación de los 2 ejemplos anteriores que hemos visto (muy inspirados en el primero).\n",
    "\n",
    "1. El codificador consta de una pila de 8 LSTM, donde el primero es bidireccional (cuyas salidas están concatenados), y existe una conexión residual entre las salidas de capas consecutivas (a partir de la 3ª capa). El decodificador es una pila separada de 8 LSTM unidireccionales.\n",
    "1. La función de puntuación utilizada es el aditivo / concat, como en [1].\n",
    "1. Nuevamente, como en [Neural Machine Translation by Jointly Learning to Align and Translate (Bahdanau et. al, 2015)](https://arxiv.org/pdf/1409.0473.pdf), la entrada al siguiente paso del decodificador es la concatenación entre la salida del paso de tiempo del decodificador anterior (rosa) y el vector de contexto del paso de tiempo actual (verde oscuro)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Traducción automática neuronal de Google"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/NMT_Google.png\" width=\"250\" height=\"500\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">NMT. Google. Las conexiones de salto se indican mediante flechas curvas. Tenga en cuenta que las celdas LSTM solo muestran el estado oculto y la entrada; no muestra la entrada del estado de la celda.</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Write a Sequence to Sequence (seq2seq) Model](https://docs.chainer.org/en/stable/examples/seq2seq.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<figure>\n",
    "<img src=\"../Imagenes/logo-final-ap.png\"  width=\"80\" height=\"80\" align=\"left\"/> \n",
    "</figure>\n",
    "\n",
    "# <span style=\"color:red\"><center>Introducción a los mecanismos de auto-atención</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Introducción a auto-atención</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué tienen en común BERT, RoBERTa, ALBERT, SpanBERT, DistilBERT, SesameBERT, SemBERT, SciBERT, BioBERT, MobileBERT, TinyBERT y CamemBERT?. NO es propiamente BERT.\n",
    "\n",
    "Respuesta: auto-atención (`self-attention`)🤗.  El modelado moderno de tareas de comprensión del lenguaje confían por completo en los mecanismos de auto-atención para generar dependencias globales entre entradas y salidas. \n",
    "\n",
    "\n",
    "La auto-atención similar a la atención.  Un mecanismo de atención siguen el siguiente algoritmo.\n",
    "\n",
    "1. Preparar entradas\n",
    "1. Inicializar los pesos\n",
    "1. Obtener `clave` (key), `consulta` (query) y `valor` (value).\n",
    "1. Calcule las puntuaciones de atención para la entrada 1.\n",
    "1. Calcular softmax de las puntuaciones.\n",
    "1. Multiplica las puntuaciones softmax con los `valores`\n",
    "1. Sumar `valores ponderados` para obtener la Salida 1\n",
    "1. Repite los pasos 4 a 7 para la entrada 2 y la entrada 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Paso 1. Preparar las entradas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las entradas correponden a una secuencia de palabras sumergidas (embebidas) en un sumergimiento (embbeding) que vamos a supone que es de tamaño 4. Para el ejemplo supondremos que que la secuencia consta de 3 palabras. Entonces tenemos 3 entradas de tamaña 4.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/self_attention_1.gif\" width=\"700\" height=\"700\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Auto-atención: Entradas.</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "Fuente: [Illustrated self attention](https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el ejemplo tenemos tres entradas.\n",
    "\n",
    "1. entrada 1: $e_1=[1, 0, 1, 0]$\n",
    "1. entrada 2: $e_2=[0, 2, 0, 2]$\n",
    "1. entrada 3: $e_3=[1, 1, 1, 1]$\n",
    "\n",
    "Matricialmente escribimos\n",
    "\n",
    "$$\n",
    "E = \\begin{pmatrix}\n",
    "1 & 0 & 1 & 0\\\\\n",
    "0 & 2 & 0 & 2\\\\\n",
    "1 & 1 & 1 &1\n",
    "\\end{pmatrix}= \\begin{pmatrix} \n",
    "e_1\\\\\n",
    "e_2\\\\\n",
    "e_3\\\\\n",
    "\\end{pmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Paso 2. Inicializar los pesos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada una de las entradas se transforman en tres diferentes vectores de algún tamaño. Para ello se cosntruyen tres transfomaciones lineales (o afines), digamos $W_k$, $W_q$ y $W_v$. Esta matrices son parámetros de la red neuronal que son inicializados aleatoriamente, o con algún procedimiento estándar de inicialización de pesos.\n",
    "\n",
    "Para esta ilustración, supondremos que las matrices de pesos son dadas por\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "W_k = \n",
    "\\begin{pmatrix} \n",
    "0 & 0 &1\\\\\n",
    "1 & 1 &0\\\\\n",
    "0 & 1 &0\\\\\n",
    "1 & 1 &0\n",
    "\\end{pmatrix}, \\quad\n",
    "W_v = \n",
    "\\begin{pmatrix} \n",
    "0 & 2 &0\\\\\n",
    "0 & 3 &0\\\\\n",
    "1 & 0 &3\\\\\n",
    "1 & 1 &0\n",
    "\\end{pmatrix}, \\quad\n",
    "W_q = \n",
    "\\begin{pmatrix} \n",
    "1 & 0 &1\\\\\n",
    "1 & 0 &0\\\\\n",
    "0 & 0 &1\\\\\n",
    "0 & 1 &1\n",
    "\\end{pmatrix}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Paso 3. Obtener claves, consultas y valores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este paso obtenemos los tres tipos de objetos derivados de la entrada: claves, consultas y valores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Claves (keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "K = E \\times W_k = \n",
    "\\begin{pmatrix} \n",
    "1 & 0 &1 &0\\\\\n",
    "0 & 2 &0 &2\\\\\n",
    "1 & 1 &1 &1\\\\\n",
    "\\end{pmatrix} \\times\n",
    "\\begin{pmatrix} \n",
    "0 & 0 &1\\\\\n",
    "1 & 1 &0\\\\\n",
    "0 & 1 &0\\\\\n",
    "1 & 1 &0\n",
    "\\end{pmatrix}  =\n",
    "\\begin{pmatrix} \n",
    "0 & 1 &1\\\\\n",
    "4 & 4 &0 \\\\\n",
    "2 & 3 &1 \\\\\n",
    "\\end{pmatrix}=\n",
    "\\begin{pmatrix} \n",
    "k_1\\\\\n",
    "k_2\\\\\n",
    "k_3\\\\\n",
    "\\end{pmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Valores (values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "V = E \\times W_v = \n",
    "\\begin{pmatrix} \n",
    "1 & 0 &1 &0\\\\\n",
    "0 & 2 &0 &2\\\\\n",
    "1 & 1 &1 &1\\\\\n",
    "\\end{pmatrix} \\times\n",
    "\\begin{pmatrix} \n",
    "0 & 2 &0\\\\\n",
    "0 & 3 &0\\\\\n",
    "1 & 0 &3\\\\\n",
    "1 & 1 &0\n",
    "\\end{pmatrix}   =\n",
    "\\begin{pmatrix} \n",
    "1 & 2 &3\\\\\n",
    "2 & 8 &0 \\\\\n",
    "2 & 6 &3 \\\\\n",
    "\\end{pmatrix}=\n",
    "\\begin{pmatrix} \n",
    "v_1\\\\\n",
    "v_2\\\\\n",
    "v_3\\\\\n",
    "\\end{pmatrix}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consultas (queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "Q = E \\times W_q = \n",
    "\\begin{pmatrix} \n",
    "1 & 0 &1 &0\\\\\n",
    "0 & 2 &0 &2\\\\\n",
    "1 & 1 &1 &1\\\\\n",
    "\\end{pmatrix} \\times\n",
    "\\begin{pmatrix} \n",
    "1 & 0 &1\\\\\n",
    "1 & 0 &0\\\\\n",
    "0 & 0 &1\\\\\n",
    "0 & 1 &1\n",
    "\\end{pmatrix}   =\n",
    "\\begin{pmatrix} \n",
    "1 & 0 &2\\\\\n",
    "2 & 2 &2 \\\\\n",
    "2 & 1 &3 \\\\\n",
    "\\end{pmatrix}=\n",
    "\\begin{pmatrix} \n",
    "q_1\\\\\n",
    "q_2\\\\\n",
    "q_3\\\\\n",
    "\\end{pmatrix}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/self_attention_2.gif\" width=\"800\" height=\"800\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Auto-atención: Obtención de claves consultas y valores.</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Illustrated self attention](https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Paso 4. Cálculo de los puntajes de auto-atención para las entradas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ilustramos aquí como se calculan los puntajes de auto-atención para la entrada 1. Los puntajes de auto-atención para las demás entradas se calculan de la misma manera con el cambio obvio.\n",
    "\n",
    "\n",
    "Como se muestra en la siguiente ilustración, se toma la consulta obtenida para la primera entrada, que en el ejemplo es $e_1 =[1, 0, 2]$. Los pesos de auto-atención son una medida de similaridad, entre la consulta y cada una de las claves, como se ha estudiado en las lecciones atención. La función de auto-atención se denotará $a$ . En este caso el resultado se obtiene haciendo el producto escalar (producto punto) entre la consulta asociada  a $e_1$, es decir, $q_1$ y cada una de las claves, es decir $a(q_1, c_i)= <q_1, c_i>, i =1,2,3$.\n",
    "\n",
    "Escrito en forma matricial, los pesos de auto-atención para la entrada 1 se calcula como\n",
    "\n",
    "\n",
    "$$\n",
    "p_1 = q_1 \\times K^T = [1, 0, 2]\\begin{pmatrix} \n",
    "0 & 4 & 2 \\\\\n",
    "1 & 4 & 3 \\\\\n",
    "1 & 0 & 1 \\\\\n",
    "\\end{pmatrix} =[2, 4, 4].\n",
    "$$\n",
    "\n",
    "Los pesos de auto-atención completos se calculan mediante\n",
    "\n",
    "$$\n",
    "P = Q \\times  K^T =\n",
    "\\begin{pmatrix} \n",
    "1 & 0 &2\\\\\n",
    "2 & 2 &2 \\\\\n",
    "2 & 1 &3 \\\\\n",
    "\\end{pmatrix} \\times\n",
    "\\begin{pmatrix} \n",
    "0 & 4 & 2 \\\\\n",
    "1 & 4 & 3 \\\\\n",
    "1 & 0 & 1 \\\\\n",
    "\\end{pmatrix}= \\begin{pmatrix} \n",
    "2 & 4 & 4 \\\\\n",
    "4 & 16 & 12 \\\\\n",
    "4 & 12 & 10 \\\\\n",
    "\\end{pmatrix} = \\begin{pmatrix} \n",
    "p_1\\\\\n",
    "p_2\\\\\n",
    "p_3\\\\\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/self_attention_3.gif\" width=\"800\" height=\"800\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Auto-atención: Obtención de puntajes de auto-atención.</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Illustrated self attention](https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 5. Cálculo del puntaje softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformamos los puntajes a la escala softmax. Por ejemplo softmax([2, 4, 4]) = [0.06, 0.47, 0.47]. Observe que softmax define una distribución discreta de probabilidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Paso 6. Multiplica los puntajes softmax con los valores para cada consulta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "S = \\begin{pmatrix}\n",
    "\\text{softmax}(p_1)\\\\\n",
    "\\text{softmax}(p_2)\\\\\n",
    "\\text{softmax}(p_3)\n",
    "\\end{pmatrix} = \n",
    "\\begin{pmatrix} \n",
    "0.06 & 0.47 &0.47\\\\\n",
    "0.00 & 0.98 &0.02 \\\\\n",
    "0.00 & 0.88 &0.12 \\\\\n",
    "\\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    "s_1\\\\\n",
    "s_2\\\\\n",
    "s_3\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la consulta 1 se tiene\n",
    "* 0.06 * [1, 2, 3] = [0.06, 0.13, 0.19]\n",
    "* 0.47 * [2, 8, 0] = [0.94, 3.75, 0.00]\n",
    "* 0.47 * [2, 6, 3] = [0.94, 2.81, 1.40]\n",
    "\n",
    "Para la consulta 2\n",
    "* 0.00 * [1, 2, 3] = [0.00, 0.00, 0.00]\n",
    "* 0.98 * [2, 8, 0] = [1.96, 7.86, 0.00]\n",
    "* 0.02 * [2, 6, 3] = [0.04, 0.11, 0.05]\n",
    "\n",
    "Para la consulta 3\n",
    "* 0.00 * [1, 2, 3] = [0.00, 0.00, 0.00]\n",
    "* 0.88 * [2, 8, 0] = [1.76, 7.04, 0.00]\n",
    "* 0.12 * [2, 6, 3] = [0.24, 0.72, 0.36]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Paso 7. Suma pesada de valores para conseguir las salidas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada sal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada salida se obtiene como la suma de los vectores pesados asociados a cada consulta así:\n",
    "\n",
    "* salida 1: [0.06, 0.13, 0.19] + [0.94, 3.75, 0.00] + [0.94, 2.81, 1.40] = [1.94, 6.68, 1.60]\n",
    "* salida 2: [0.00, 0.00, 0.00] + [1.96, 7.86, 0.00] + [0.04, 0.11, 0.05] = [2.00, 7.96, 0.05]\n",
    "* salida 3: [0.00, 0.00, 0.00] + [1.76, 7.04, 0.00] + [0.24, 0.72, 0.36] = [2.00, 7.76, 0.36]\n",
    "\n",
    "El siguiente gif muestra el modelo completo de auto-atención. Los valores se redondearon. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/self_attention_4.gif\" width=\"800\" height=\"800\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Auto-atención: Cálculo completo de las últimas dos salidas</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Illustrated self attention](https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
